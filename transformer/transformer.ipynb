{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원문 : \n",
    "[pytorch로 구현하는 Transformer (Attention is All You Need)](https://cpm0722.github.io/pytorch-implementation/transformer)\n",
    "\n",
    "paper: [Attention is All You Need](https://arxiv.org/pdf/1706.03762.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 개괄 구조\n",
    "\n",
    "sentence input -> sentence output.\n",
    "\n",
    "- 어떤 형태의 인풋이든, 어떤 형태의 아웃풋이든 정하기에 따라 다르게 쓸 수 있다. (동일 sentence 출력 / 번역 / 역방향 / ...)\n",
    "\n",
    "인코더 & 디코더\n",
    "\n",
    "- 인코더 : input sentence를 Context('문맥'을 함축한 하나의 벡터)로. 정보를 빠뜨리지 않고 압축하는게 목표.\n",
    "- 디코더 : Context & Some sentence를 output sentence로."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Transformer, self).__init__() # Python2의 문법이지만 범용성을 위해 흔히 이렇게 사용한다.\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, x, z):\n",
    "        c = self.encoder(x) # context를 내놓는 인코더\n",
    "        y = self.decoder(z, c) # context와 어떤 sentence를 받아 output을 내놓는 디코더\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Encoder의 인풋은 문장 임베딩. 인코더 내부에는 Encoder Layer 여러개로 구성된다. \n",
    "    - 논문에서는 6개 Layer 사용\n",
    "    - 각 Layer는 input에 대해 더 높은 차원(넓은 관점 or 추상적)의 context를 담는다.\n",
    "Encoder Layer는 같은 shape을 사용해야하고, 결국 input과 context의 shape도 같다: 여러개의 Layer가 연속적으로 input-output으로 이어지기때문.\n",
    "\"\"\"\n",
    "import copy\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, encoder_layer, n_layer):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = []\n",
    "        for i in range(n_layer):\n",
    "            self.layers.append(copy.deepcopy(encoder_layer))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for layer in self.layers:\n",
    "            out = layer(out) # 순서대로 앞의 output을 다음 input으로.\n",
    "        return out\n",
    "\n",
    "\"\"\"\n",
    "Encoder Layer는 다음으로 구성:\n",
    "    - Multi-Head Attention Layer\n",
    "    - Position-wise Feed-Forward Layer\n",
    "\"\"\"\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, multi_head_attention_layer, position_wise_feed_forward_layer):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.multi_head_attention_layer = multi_head_attention_layer\n",
    "        self.position_wise_feed_forward_layer = position_wise_feed_forward_layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.multi_head_attention_layer(x)\n",
    "        out = self.position_wise_feed_forward_layer(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention?\n",
    "\n",
    "Multi-Head Attention은 Self-Attention을 병렬로 수행하는 Layer. Self-Attention은 문장 내 두 토큰 사이의 연관성을 찾아내는 방법론이다. (Self: 같은 문장 내를 의미, 여기에서 다른 토큰 간 Attention)\n",
    "\n",
    "### RNN과 비교한 Self-Attention\n",
    "\n",
    "RNN : 이전 시점까지의 토큰에 대한 hidden state 내부에 이전 정보가 저장됨. 이전 정보를 통해 연관성을 찾는다는 개념 자체는 동일하나, Self-Attention의 장점은:\n",
    "\n",
    "    1. i번째 hidden state를 계산하기 위해 i-1번째 hidden state 필요...-> 병렬처리 불가능. 하지만 Self-Attention은 모든 token 쌍 사이의 attention을 한 번의 Matrix 곱연산으로 구해내어 병렬처리 가능\n",
    "    2. RNN은 시간이 지남(시퀀스 진행)에 따라 이전 토큰 정보가 희미해짐, 먼 거리 토큰의 연관성 정보가 제대로 반영되지 않음. 반면 Self-Attention은 각 토큰간 관계를 direct로 구해내므로 보다 명확하게 관계를 잡아낸다.\n",
    "\n",
    "### Query, Key, Value\n",
    "\n",
    "1. Query : 현재 시점의 토큰. **\"Attention\"이란 Query의 어탠션이다!**\n",
    "2. key : attention을 구하고자 하는 대상 토큰\n",
    "3. Value : attention을 구하고자 하는 대상 토큰(Key와 동일 토큰)\n",
    "\n",
    "- 기본 로직 : Query에 가장 부합하는(=attention이 높은) -> Key&Value 대상 토큰을 찾는다.\n",
    "- Key&Value의 차이점? Key와 Value가 가리키는 토큰은 같은 토큰(Query가 찾는 토큰)인데, 추후 계산을 위해 이렇게 두 가지 값이 존재한다.\n",
    "- 세 개의 벡터는 각각 서로 다른 Fully-Connected Layer로 생성된다.(모두 같은 input&output dimension과 shape)\n",
    "- dimension : d<sub>k</sub> (--> Key의 차원수라는 말인데, 굳이 Key에 의미부여한건 아니다. 그냥 다 같은 값인데 이름을 Key로 붙인 것)\n",
    "\n",
    "#### 계산 방법\n",
    "\n",
    "Query * Key 행렬곱 = Attention Score. 이를 스칼라 값인 차원수(d<sub>k</sub>)로 나눠준다(grad vanishing 방지)\n",
    "<br>\n",
    "\n",
    "**-> 각 토큰이 쿼리와 얼마나 attention을 갖는지(연관성을 갖는지)를 표현해준다**\n",
    "\n",
    "실제 계산할 때는 Query에 대해 문장 내 모든 토큰에 대한 attention을 구해야 한다 : 행렬곱!<br>\n",
    "**-> Key와 Value는 전체 문장인 Matrix가 된다(각 토큰 벡터를 갖는 문장 행렬)**\n",
    "\n",
    "Query * Key인 Attention Score에 SoftMax를 취하고, Value와 다시 행렬곱 = Query's Attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad Masking\n",
    "\n",
    "어탠션 계산 중 d<sub>k</sub> 스케일링과 SoftMax 적용 사이에 Masking을 수행한다.\n",
    "\n",
    "트랜스포머에도 다른 시퀀스 모델들처럼 input 시퀀스를 같은 수로 맞춰주기 위한 패딩이 들어가는데, **아무 정보가 없는 패딩에 어탠션을 부여하면 안되므로 pad masking을 수행**한다.\n",
    "\n",
    "계산 방법 : masking을 위한 Matrix를 곱한다 => **패딩 토큰에 해당하는 요소는 모두 -inf, 그 외에는 1**인 행렬."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_attention(self, query, key, value, mask):\n",
    "    # Q, K, V shape : (batch_size, seq_len, d_k)\n",
    "    d_k = key.size(-1) # key의 차원\n",
    "\n",
    "    # attention_score shape : (batch_size, seq_len, seq_len)\n",
    "    attention_score = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k) # Q X K^T / sqrt(d_k)\n",
    "\n",
    "    if mask is not None:\n",
    "        attention_score = attention_score.masked_fill(mask == 0, -1e9) # masking\n",
    "\n",
    "    # attention_prob shape : (batch_size, seq_len, seq_len)\n",
    "    attention_prob = F.softmax(attention_score, dim = -1) # softmax\n",
    "\n",
    "    # output shape : (batch_size, seq_len, d_k)\n",
    "    out = torch.matmul(attention_prob, value) # attention_prob X V\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention Layer\n",
    "\n",
    "이제 Self-Attention을 이해했으니 다음 단계인 Multi-Head Attention을 이해해보자. 논문에서는 **Scaled Dot-Product Attention**이라고 부른다. 트랜스포머는 Scaled Dot Attention을 각 Encoder Layer마다 여러번(<i>h</i>번) 수행하여 그 결과를 종합한다.\n",
    "<br>\n",
    "--> 여러 Attention을 잘 반영하기 위해서이다. 한 번만 진행한 Attention이라면 강력하게 연관된 토큰들만 엮여서, 문장 내 함축된 모든 정보를 담아내기에는 힘들다.\n",
    "\n",
    "**d<sub>model</sub>** : Scaled Dot-Product Attention 연산의 핵심 개념. \n",
    "- 이제 Q,K,V에 대해 여러번(<i>h</i>번) 수행하므로 3*<i>h</i>개의 FC Layer가 필요하다. \n",
    "- 이것으로 d<sub>k</sub> X <i>h</i> 형태의 모델을 형성한다. (실제 어탠션 Matrix의 전체 shape은 max_seq_len * (d<sub>k</sub> X <i>h</i>))\n",
    "- 효율적인 병렬연산을 위해 Q,K,V 벡터를 따로 생성(d<sub>embed</sub> X d<sub>k</sub> 행렬을 3*<i>h</i>개)하지 않고, 한번에 d<sub>embed</sub> X d<sub>model</sub>의 weight matrix를 갖는 3개의 FC Layer를 만들게 된다.\n",
    "\n",
    "==> 즉, 결론만 놓고 보면 Self-Attention과의 차이는 차원수 d<sub>k</sub>를 d<sub>model</sub>로 단순 확장한 것. **정보의 양을 더 많이 담는 차원으로 Q,K,V의 차원을 확장한 것이다.**\n",
    "\n",
    "==> 실제 계산은 아래 transform()에서 보이듯 reshape하여 d<sub>model</sub>을 <i>h</i>와 d<sub>k</sub>로 분해해 차원을 늘리고, transpose한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 중간 점검 : 용어들 정리\n",
    "\n",
    "- max_seq_len OR <i>n</i>\n",
    "- Q, K, V : 각각 Query, Key, Value에 해당하는 벡터 또는 행렬. 아래는 batch_size 없을 때 크기 예시)\n",
    "    - Q = Input sentence(<i>n</i> X d<sub>embed</sub>) X Query FC Layer(d<sub>embed</sub> X d<sub>k</sub>)\n",
    "    - K = Input sentence(<i>n</i> X d<sub>embed</sub>) X Key FC Layer(d<sub>embed</sub> X d<sub>k</sub>)\n",
    "    - V = Input sentence(<i>n</i> X d<sub>embed</sub>) X Value FC Layer(d<sub>embed</sub> X d<sub>k</sub>)\n",
    "- pad masking : 패딩 정보 제거용 행렬. 크기는 Q X K<sup>T</sup>를 수행한 이후와 같은 <i>seq_len</i> X <i>seq_len</i>\n",
    "- d<sub>k</sub> : Q,K,V의 차원 수\n",
    "- d<sub>embed</sub> : 토큰의 embedding dimension. 사실상 임베딩을 거쳐 나오는 이 차원수는 d<sub>model</sub>과 같은 값\n",
    "- <i>h</i> : 헤드 수. 어탠션 수행할 횟수\n",
    "- d<sub>model</sub> : Multi-Head Attention의 전체 FC Layer에 입력할 차원 수. d<sub>k</sub> * <i>h</i>\n",
    "    - Multi-Head Attention의 FC Layer shape : d<sub>model</sub> X d<sub>embed</sub>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, d_model, h, qkv_fc_layer, fc_layer):\n",
    "        # qkv_fc_layer shape : (d_embed, d_model)\n",
    "        # fc_layer shape : (d_model, d_embed)\n",
    "        super(MultiHeadAttentionLayer, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.h = h\n",
    "        self.query_fc_layer = copy.deepcopy(qkv_fc_layer)\n",
    "        self.key_fc_layer = copy.deepcopy(qkv_fc_layer)\n",
    "        self.value_fc_layer = copy.deepcopy(qkv_fc_layer)\n",
    "        self.fc_layer = fc_layer\n",
    "\n",
    "    # !!Transformer의 핵심인 forward 함수!!\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        # query, key, value 인자가 실제로 실제 Q,K,V 행렬이 아니라, 각각의 FC Layer로 입력할 input sentence\n",
    "        # query, key, value shape : (batch_size, seq_len, d_embed)\n",
    "        # mask shape : (batch_size, seq_len, seq_len)\n",
    "        batch_size = query.shape[0]\n",
    "\n",
    "        # x shape : (batch_size, seq_len, d_embed)\n",
    "        # reshape to (batch_size, seq_len, h, d_k)\n",
    "        def transform(x, fc_layer): # Q,K,V 행렬 구하기\n",
    "            out = fc_layer(x) # (batch_size, seq_len, d_model)\n",
    "            out = out.view(batch_size, -1, self.h, self.d_model//self.h) # (batch_size, seq_len, h, d_k)\n",
    "            out = out.transpose(1, 2) # (batch_size, h, seq_len, d_k)\n",
    "            return out # 이렇게 형태를 변형하는 이유는 아래 calculate_attention 함수에서 사용하기 위해서.\n",
    "\n",
    "        # query, key, value shape after transform : (batch_size, h, seq_len, d_k)\n",
    "        query = transform(query, self.query_fc_layer)\n",
    "        key = transform(key, self.key_fc_layer)\n",
    "        value = transform(value, self.value_fc_layer)\n",
    "\n",
    "        if mask is not None:\n",
    "            # 아래 calculate_attention에서 마스킹을 브로드캐스팅으로 수행하기 위해 변형해준다.\n",
    "            mask = mask.unsqueeze(1) # (batch_size, 1, seq_len, seq_len)\n",
    "\n",
    "        out = self.calculate_attention(query, key, value, mask) # out shape : (batch_size, h, seq_len, d_k)\n",
    "        out = out.transpose(1, 2) # (batch_size, seq_len, h, d_k)\n",
    "        \"\"\"\n",
    "        contiguous : 연속적인 메모리 텐서를 할당해주는 함수로, \n",
    "        차원을 변형할때 실제론 메타정보만 바뀌는 view 같은 함수에 활용하면 다음 메모리로 넘어가는 \n",
    "        stride가 고정되지 않고 잘 할당된다는 것 같다.\n",
    "        \"\"\"\n",
    "        out = out.contiguous().view(batch_size, -1, self.d_model) # (batch_size, seq_len, d_model)\n",
    "        out = self.fc_layer(out) # (batch_size, seq_len, d_embed), 인풋 형태로 복귀\n",
    "\n",
    "        return out\n",
    "\n",
    "    \"\"\"\n",
    "    Self-Attention때와 똑같은 함수지만, 해당 함수의 input & output shape이 다름에 주의.\n",
    "        -> h에 의해 차원수가 하나 늘어난 정도로, 연산 자체는 브로드캐스팅을 활용하면 완전히 똑같다.\n",
    "    (*참고) matmul의 연산방식 : \"만약 배열이 2차원보다 클 경우, 마지막 2개의 축으로 이루어진 행렬을 나머지 축에 따라 쌓아놓은 것이라고 간주한다.\"\n",
    "        즉, (1) 맨 뒤 두 개 차원으로 행렬곱이 가능한 경우 & (2) 맨 뒤 두 개 외에 나머지 차원축 상의 갯수가 같은 경우\n",
    "    \"\"\"\n",
    "    def calculate_attention(self, query, key, value, mask):\n",
    "        # Self-Attention에서의 Q, K, V shape : (batch_size, seq_len, d_k)\n",
    "        # 여기에서의 Q, K, V shape : (batch_size, h, seq_len, d_k)\n",
    "        d_k = key.size(-1)\n",
    "        \n",
    "        # attention_score shape : (batch_size, seq_len, seq_len)\n",
    "        attention_score = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k) # Q X K^T / sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            attention_score = attention_score.masked_fill(mask == 0, -1e9)\n",
    "        attention_prob = F.softmax(attention_score, dim = -1) # softmax\n",
    "        out = torch.matmul(attention_prob, value) # attention_prob X V\n",
    "\n",
    "        return out # out shape : (batch_size, h, seq_len, d_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs\n",
    "### Positional Encoding\n",
    "\n",
    "기본적인 단순 embedding에 더해, Transformer의 embedding은 PositionalEncoding이 더해진다\n",
    "- 즉, token embedding + PositionalEncoding의 Sequential 모델이라 할 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    단순 embedding + PositionalEncoding 전체 연결을 정의\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding, positional_encoding):\n",
    "        super(TransformerEmbedding, self).__init__(): # 상속 클래스 init\n",
    "        self.embedding = nn.Sequential(embedding, positional_encoding) # 기존 embedding + PositionalEncoding\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.embedding(x)\n",
    "        return out\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    \"\"\"\n",
    "    단순 embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, d_embed, vocab):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(len(vocab), d_embed)\n",
    "        self.vocab = vocab\n",
    "        self.d_embed = d_embed\n",
    "    def forward(self, x):\n",
    "        out = self.embedding(x) * math.sqrt(self.d_embed) # scaling : PositionalEncoding의 영향을 상대적으로 적게 가져가기 위해 단순 임베딩의 크기를 늘린다.\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PositionalEncoding 클래스**\n",
    "\n",
    "PositionalEncoding의 목적 : 위치 정보(=토큰 순서 =index num)를 정규화시키기\n",
    "- 단순히 index num으로 쓰게 된다면? : 새로운 test input의 길이가 positional encoding이 본 적 없는 긴 길이일때 문제가 생긴다..\n",
    "- 따라서 sin & cos 함수를 써서 [-1,1] 안으로 제한하는 것\n",
    "\n",
    "아래 수식은 논문 수식과 약간은 차이가 있는거 같은데(자연상수 e를 밑으로 하는 div_term과 달리 그냥 10을 밑으로 함?)\n",
    "아마 큰 차이는 없는거 아닐까?\n",
    "\n",
    "수식 div_term : $$e^{-\\log 10000 \\times 2i/d_{embed}}=e^{-\\log 10000} \\cdot e^{2i/d_{embed}}=$$\n",
    "$$ {1 \\over 10000} \\cdot e^{2i/d_{embed}}$$\n",
    "수식 (1) (짝수) : $$ \\sin ({{pos \\times e^{2i/d_{embed}}} \\over 10000})$$\n",
    "수식 (2) (홀수) : $$ \\cos ({{pos \\times e^{2i/d_{embed}}} \\over 10000})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_embed, max_seq_len=5000):\n",
    "        super().__init__()\n",
    "        encoding = torch.zeros(max_seq_len, d_embed) # 임베딩 최종 결과물 shape인 (n, d_embed)\n",
    "        position = torch.arange(0, max_seq_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_embed, 2) * -(math.log(10000.0) / d_embed)) # 수식 div_term\n",
    "        encoding[:, 0::2] = torch.sin(position * div_term) # 수식 (1)\n",
    "        encoding[:, 1::2] = torch.cos(position * div_term) # 수식 (2)\n",
    "        self.encoding = encoding\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x + Variable(self.encoding[:, :x.size(1)], requires_grad=False) # 여기 forward에서 Variable이 학습되지 않도록 한다."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9b5b6fe6f6f8ee886c0373eea8c73e9a18de4cae99809543e0fa23dcdea02fdf"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('nlp': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
